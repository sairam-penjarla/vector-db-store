{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Vectors, Tokens, and Efficient Indexing with LlamaIndex and ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **vector** is a numerical representation of data, often used to capture the meaning of words or phrases in machine learning models. For example, the sentence \"I like apple\" can be converted into a vector like `[-1.0, -0.0002, 4]`, where each number represents a specific aspect of the sentence’s meaning or structure. A **token** is a fundamental unit of text, such as a word or part of a word, that the model processes. In the vector `[-1.0, -0.0002, 4]`, each number corresponds to a token from the sentence \"I like apple\", where `-1.0` could represent the token \"I\", `-0.0002` could represent \"like\", and `4` could represent \"apple\". Tokens are converted into vectors for efficient processing by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"I like apple\"\n",
    "\n",
    "# vector\n",
    "[-1.0, -0.0002, 4]\n",
    "\n",
    "-1.0 # token\n",
    "-0.0002 # token\n",
    "4 # token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sentence Embeddings with Pretrained Models\n",
    "\n",
    "In this code, we're using a pre-trained model from the `SentenceTransformers` library to convert sentences into vector representations known as embeddings. First, the `SentenceTransformer` model is loaded with the pre-trained model `all-MiniLM-L6-v2`, a lightweight model that generates meaningful embeddings for sentences. Then, a list of sentences (`lines`) is defined, and the `model.encode()` method is applied to this list to convert each sentence into a fixed-size embedding, a high-dimensional vector capturing the semantic meaning of the sentence. These embeddings can be used in various NLP tasks such as text similarity, clustering, or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/writer_env/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01040684  0.01423134  0.02288207 ...  0.0235445  -0.0365877\n",
      "   0.01918398]\n",
      " [ 0.02909455 -0.0835634  -0.02009496 ...  0.06541307 -0.07751977\n",
      "   0.04729676]\n",
      " [-0.04539156 -0.04705414 -0.0403274  ...  0.0944678   0.07234132\n",
      "   0.01783809]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "lines = [\n",
    "    \"Biryani is a flavorful rice dish made with fragrant basmati rice and spices.\",\n",
    "    \"Samosas are crispy pastries filled with spiced potatoes and peas.\",\n",
    "    \"Butter chicken is a creamy curry made with marinated chicken and a rich tomato sauce.\",\n",
    "]\n",
    "\n",
    "\n",
    "embeddings = model.encode(lines)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Similarity Between Sentence Embeddings\n",
    "\n",
    "In this snippet, a new sentence (user_question) is embedded using the same pre-trained model (SentenceTransformer). After generating its embedding (question_embeddings), the similarity between this embedding and the previously encoded sentences (embeddings) is computed. Since SentenceTransformer does not have a built-in similarity() method, you would typically compute cosine similarity between vectors manually using a library like numpy or scipy. Cosine similarity measures how similar two vectors are, based on the angle between them, where 1 means the vectors are identical and -1 means they are completely opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9277],\n",
      "        [0.3249],\n",
      "        [0.2918]])\n"
     ]
    }
   ],
   "source": [
    "user_question = [\"Biryani is a flavorful rice dish made \"]\n",
    "\n",
    "question_embeddings = model.encode(user_question)\n",
    "\n",
    "similarities = model.similarity(embeddings, question_embeddings)\n",
    "\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and Managing Documents with ChromaDB\n",
    "\n",
    "This code snippet demonstrates how to use ChromaDB, a vector database for storing and managing text-based documents along with their embeddings. First, a client is initialized using `chromadb.Client()` to interact with the database. Next, the code retrieves or creates a collection named `\"my_collection1\"` using the `get_or_create_collection` method. Collections in ChromaDB serve as containers to hold documents and their related metadata. The `upsert()` function is used to insert or update documents, where two simple text documents about \"pineapple\" and \"oranges\" are added, each associated with unique identifiers (`\"id11\"` and `\"id21\"`). This operation allows storing, updating, and retrieving information in a structured manner for later use, such as searching for similar documents or performing semantic queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"indian_food_1\")\n",
    "\n",
    "collection.upsert(\n",
    "    documents=[\n",
    "        \"This is a doasdasdacument about pineapple\",\n",
    "        \"This is a document dfsdfabout oranges\"\n",
    "    ],\n",
    "    ids=[\"id11\", \"id21\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sentence Embeddings with ChromaDB\n",
    "\n",
    "In this snippet, you're importing the `embedding_functions` module from ChromaDB to use a pre-trained sentence transformer model for generating embeddings. Specifically, you're creating an `embedding_function` object (`sentence_transformer_ef`) using the `SentenceTransformerEmbeddingFunction` class, and loading the `\"all-MiniLM-L6-v2\"` model. This model is responsible for converting textual data into meaningful vector representations (embeddings). These embeddings can later be stored in a ChromaDB collection and used for tasks like document similarity, semantic search, or clustering. By integrating this function with ChromaDB, you can efficiently manage and query vectorized data for advanced text-based applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Documents with Metadata and Embeddings into ChromaDB\n",
    "\n",
    "This code inserts documents into a ChromaDB collection along with their embeddings and associated metadata. It processes a list of documents, metadata, and unique IDs using a loop. For each document, the corresponding embedding is generated using the `sentence_transformer_ef` function, which converts the text into a vector representation. Each document, along with its embedding, metadata (such as the source of the document, e.g., Notion or Google Docs), and its unique ID, is added to the collection using the `upsert()` method. This allows the ChromaDB collection to store both the content and related information for later retrieval or analysis, making it useful for applications like semantic search or document management systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Biryani is a popular Indian dish made with fragrant basmati rice, spices, and marinated meat.\",\n",
    "    \"Paneer Tikka is a vegetarian dish consisting of marinated paneer (cottage cheese) grilled to perfection.\"\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": \"recipe_book\"},\n",
    "    {\"source\": \"food_blog\"}\n",
    "]\n",
    "ids = [\"dish1\", \"dish2\"]\n",
    "\n",
    "\n",
    "for doc, meta, id in zip(documents, metadatas, ids):\n",
    "    collection.upsert(\n",
    "        documents=[doc],\n",
    "        embeddings=[sentence_transformer_ef(doc)[0]],\n",
    "        metadatas=[meta],\n",
    "        ids=[id],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Documents in ChromaDB\n",
    "\n",
    "In this code, a query is performed on the ChromaDB collection to find the top 2 most relevant documents based on the query text. The `collection.query()` method is used, where `query_texts` contains the text to search for, in this case, \"This is a query document about florida\". The parameter `n_results=2` specifies that the top 2 closest matches (based on vector similarity) should be returned. ChromaDB will internally compute the similarity between the query text and the stored documents using their embeddings. The result, which is printed at the end, will contain information about the most similar documents, potentially including their content, embeddings, metadata, and similarity scores. \n",
    "\n",
    "This approach is particularly useful for semantic search tasks where you want to retrieve documents based on their meaning rather than keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['dish1', 'dish2']], 'embeddings': None, 'documents': [['Biryani is a popular Indian dish made with fragrant basmati rice, spices, and marinated meat.', 'Paneer Tikka is a vegetarian dish consisting of marinated paneer (cottage cheese) grilled to perfection.']], 'uris': None, 'data': None, 'metadatas': [[{'source': 'recipe_book'}, {'source': 'food_blog'}]], 'distances': [[1.8978359699249268, 1.917147159576416]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = collection.query(\n",
    "    query_texts = [\"What are the must-try dishes in Indian cuisine?\"],\n",
    "    n_results=2 \n",
    ")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Long Text into Manageable Chunks\n",
    "\n",
    "In this code snippet, the `RecursiveCharacterTextSplitter` from LangChain is utilized to break down the `long_text` into smaller, more manageable pieces (chunks). The `splitter` is initialized with two parameters: `chunk_size=200`, which specifies that each chunk should ideally contain up to 200 characters, and `chunk_overlap=20`, which indicates that each chunk will overlap with the next by 20 characters. This overlap can help maintain context between chunks, making it useful for tasks like natural language processing or when working with large text inputs where context preservation is important. After initializing the splitter, the `split_text(long_text)` method is called, which processes the `long_text` and produces a list of chunks that can be more easily analyzed or processed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian cuisine is known for its diverse flavors and rich cultural heritage. Dishes like Biryani, a fragrant rice dish cooked with spices and marinated meat, are popular across the country. Paneer Tikka, made from marinated cottage cheese grilled to perfection, is a favorite among vegetarians. Indian food also includes an array of delicious curries, such as Butter Chicken, which is known for its creamy tomato sauce. Street food like Pani Puri and Samosas are beloved snacks that offer a burst of flavors. The culinary traditions vary from region to region, making Indian cuisine a delightful experience for food lovers.\n"
     ]
    }
   ],
   "source": [
    "long_text = (\n",
    "    \"Indian cuisine is known for its diverse flavors and rich cultural heritage. \"\n",
    "    \"Dishes like Biryani, a fragrant rice dish cooked with spices and marinated meat, are popular across the country. \"\n",
    "    \"Paneer Tikka, made from marinated cottage cheese grilled to perfection, is a favorite among vegetarians. \"\n",
    "    \"Indian food also includes an array of delicious curries, such as Butter Chicken, which is known for its creamy tomato sauce. \"\n",
    "    \"Street food like Pani Puri and Samosas are beloved snacks that offer a burst of flavors. \"\n",
    "    \"The culinary traditions vary from region to region, making Indian cuisine a delightful experience for food lovers.\"\n",
    ")\n",
    "\n",
    "print(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = splitter.split_text(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Indian cuisine is known for its diverse flavors and rich cultural heritage. Dishes like Biryani, a'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rich cultural heritage. Dishes like Biryani, a fragrant rice dish cooked with spices and marinated'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Embeddings Class for Sentence Encoding\n",
    "\n",
    "In this code, a custom class `CustomEmbeddings` is defined, inheriting from the `Embeddings` base class in LangChain. This class allows you to leverage the `SentenceTransformer` model for embedding text documents and queries. The constructor (`__init__`) initializes the class by loading a specified sentence transformer model using its name. \n",
    "\n",
    "The `embed_documents` method takes a list of documents as input and returns their embeddings as a list of lists, where each inner list represents the embedding for a specific document. It uses the `model.encode()` method to generate embeddings and converts the results to lists for easier handling.\n",
    "\n",
    "The `embed_query` method is designed to embed a single query string, returning its embedding as a list. It also utilizes the `model.encode()` method, but it processes the query as a single-item list before extracting the first (and only) embedding.\n",
    "\n",
    "This custom implementation enables seamless integration of sentence embeddings into LangChain workflows, allowing for enhanced natural language understanding tasks such as document similarity and semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return [self.model.encode(d).tolist() for d in documents]\n",
    "\n",
    "    def embed_query(self, query: str):\n",
    "        return self.model.encode([query])[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Document Instances with Unique Identifiers\n",
    "\n",
    "In this code, a collection of `Document` instances is created to represent various text entries, each associated with specific metadata and a unique identifier. The `Document` class is imported from `langchain_core.documents`. Each document includes the following:\n",
    "\n",
    "- **`page_content`**: The actual text content of the document, which can range from tweets to news articles.\n",
    "- **`metadata`**: A dictionary providing context about the source of the document, such as \"tweet,\" \"news,\" or \"website.\"\n",
    "- **`id`**: A numeric identifier for the document.\n",
    "\n",
    "Ten documents are instantiated with varying content, and they are grouped into a list called `documents`. Additionally, a list of unique identifiers (`uuids`) is generated using the `uuid4` function from the `uuid` module. This function creates a random UUID (Universally Unique Identifier) for each document, ensuring that each document can be uniquely identified in a larger system. This structure is useful for organizing and managing text data, enabling efficient retrieval and processing in applications like semantic search or document analysis. \n",
    "\n",
    "Note: The `id` field in each `Document` is currently set to an integer value, while the `uuids` list generates string representations of UUIDs. Depending on your application, you might want to use one or the other consistently for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had delicious Biryani for lunch, and it was bursting with flavors!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather is perfect for enjoying a plate of spicy Samosas.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Trying out a new recipe for Paneer Tikka tonight - can't wait to share it!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"The restaurant introduced a new Butter Chicken dish, and it has become an instant hit.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Just had an amazing meal at an Indian restaurant. The flavors were out of this world!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new recipe for Masala Dosa worth trying? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"Here are the top 10 Indian sweets you must try this festival season.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is a great tool for sharing Indian food recipes with others!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The Indian spice market is buzzing with activity as festival season approaches.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I can't believe how much I love Indian food - it's simply addictive!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Vector Store with ChromaDB and Custom Embeddings\n",
    "\n",
    "In this code snippet, a vector store is being set up using ChromaDB and a custom embedding model. Here’s a breakdown of the process:\n",
    "\n",
    "1. **Importing Required Libraries**: The code imports the necessary libraries, including `chromadb` for interacting with the Chroma database and `Chroma` from `langchain.vectorstores` for creating the vector store.\n",
    "\n",
    "2. **Creating a Custom Embedding Model**: An instance of `CustomEmbeddings` is created, using the model `\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"`. This model is designed to generate multilingual sentence embeddings that can capture semantic similarities across different languages.\n",
    "\n",
    "3. **Initializing a Persistent ChromaDB Client**: The `PersistentClient` from ChromaDB is instantiated, allowing you to create or access a persistent collection of documents and their embeddings.\n",
    "\n",
    "4. **Creating or Accessing a Collection**: The `get_or_create_collection` method is called on the persistent client to either retrieve an existing collection named `\"collection_name\"` or create a new one if it doesn’t already exist.\n",
    "\n",
    "5. **Setting Up the Vector Store**: Finally, a `Chroma` vector store is created, linking the persistent client, the specified collection, and the custom embedding function. This vector store can now be used to store, manage, and query embeddings for documents efficiently.\n",
    "\n",
    "By integrating a custom embedding model with ChromaDB, you enable advanced capabilities for semantic search, document retrieval, and other natural language processing tasks, making it easier to handle and analyze large sets of textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/wjjg1k5d3nb_gh0ttw8xz_f40000gn/T/ipykernel_2173/946615754.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_model = CustomEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"indian_food\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"indian_food\",\n",
    "    embedding_function=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b6e9b472-dde9-4cf7-9f42-58360882bd52',\n",
       " '815acdaf-a02a-4891-8130-9580f64eda22',\n",
       " '96aa33ba-4cbd-4c32-8555-d1449d08518d',\n",
       " 'eb012bee-87c2-42e6-a3dd-2dd8bc8c2fbf',\n",
       " '88e0932c-a333-4796-90a1-0d936aec9323',\n",
       " '7449695f-da7d-453c-98fe-4423bd23e1f7',\n",
       " '8108e9e9-582d-42ec-ac90-2d8aa0748d3a',\n",
       " '4242695b-eeb3-4807-bc92-c5d287be2bfc',\n",
       " 'b5108df3-6f37-4088-9863-e1362d10aec6',\n",
       " 'd757c154-0178-4a3d-8af1-e26122829ddf']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a Similarity Search in ChromaDB\n",
    "\n",
    "In this code snippet, a similarity search is executed on the `vector_store` created earlier to find documents that are semantically similar to a provided query. Here’s how it works:\n",
    "\n",
    "1. **Querying for Similar Documents**: The `similarity_search` method of the `vector_store` is called with the query text `\"LangChain provides abstractions to make working with LLMs easy\"`. The parameter `k=2` specifies that the search should return the top 2 most similar documents.\n",
    "\n",
    "2. **Applying a Filter**: A filter is applied to the search results using the `filter` parameter, which restricts the search to documents where the `source` is `\"tweet\"`. This helps narrow down the results to a specific category of documents, ensuring that only relevant entries are considered.\n",
    "\n",
    "3. **Displaying Results**: The results of the search are iterated over, and for each result, the content of the document (`res.page_content`) and its associated metadata (`res.metadata`) are printed in a formatted manner. This output gives insights into the documents that are most relevant to the query while displaying their source information.\n",
    "\n",
    "This process demonstrates how to efficiently retrieve contextually relevant information from a vector store, allowing for dynamic querying and enhancing the capabilities of applications built with LangChain and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* LangGraph is a great tool for sharing Indian food recipes with others! [{'source': 'tweet'}]\n",
      "* Just had an amazing meal at an Indian restaurant. The flavors were out of this world! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"What are some popular dishes in Indian cuisine?\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['dish1', 'dish2']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Biryani is a popular Indian dish made with fragrant basmati rice, spices, and marinated meat.',\n",
       "   'Paneer Tikka is a vegetarian dish consisting of marinated paneer (cottage cheese) grilled to perfection.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'source': 'recipe_book'}, {'source': 'food_blog'}]],\n",
       " 'distances': [[1.8978359699249268, 1.917147159576416]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The culinary traditions vary from region to region, making Indian cuisine a delightful experience for food lovers.', 'Indian food also includes an array of delicious curries, such as Butter Chicken, which is known for its creamy tomato sauce. ', 'Street food like Pani Puri and Samosas are beloved snacks that offer a burst of flavors. ']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import chromadb\n",
    "\n",
    "\n",
    "# Step 1: Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Step 2: Create a collection in ChromaDB\n",
    "collection = client.get_or_create_collection(\"documents_collection\")\n",
    "\n",
    "# Step 3: Function to embed documents using OpenAI API\n",
    "embedding_model = CustomEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "def embed_document(text):\n",
    "    return embedding_model.embed_query(text)  \n",
    "\n",
    "# Step 4: Add documents to ChromaDB with their embeddings\n",
    "def add_documents_to_collection(documents):\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        embedding = embed_document(text)  # Generate embedding for the document\n",
    "        collection.add(\n",
    "            documents=[text],\n",
    "            embeddings=[embedding],\n",
    "            ids=[str(doc_id)]\n",
    "        )\n",
    "\n",
    "# Step 5: Query ChromaDB to find the most relevant documents\n",
    "def query_chromadb(query, top_n=3):\n",
    "    query_embedding = embed_document(query)  # Embed the user query\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_n)\n",
    "    return results[\"documents\"]\n",
    "\n",
    "\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Indian cuisine is known for its diverse flavors and rich cultural heritage. \",\n",
    "    \"Dishes like Biryani, a fragrant rice dish cooked with spices and marinated meat, are popular across the country. \",\n",
    "    \"Paneer Tikka, made from marinated cottage cheese grilled to perfection, is a favorite among vegetarians. \",\n",
    "    \"Indian food also includes an array of delicious curries, such as Butter Chicken, which is known for its creamy tomato sauce. \",\n",
    "    \"Street food like Pani Puri and Samosas are beloved snacks that offer a burst of flavors. \",\n",
    "    \"The culinary traditions vary from region to region, making Indian cuisine a delightful experience for food lovers.\",\n",
    "]\n",
    "\n",
    "# Step 1: Add documents to the ChromaDB collection\n",
    "add_documents_to_collection(documents)\n",
    "\n",
    "# Step 2: Query the database and retrieve relevant documents\n",
    "user_query = \"What indian cuisine is known for its rich cultural heritage?\"\n",
    "relevant_docs = query_chromadb(user_query)\n",
    "\n",
    "print(relevant_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_answer_from_docs(query, documents):\n",
    "    context = \"\\n\".join(documents)  # Concatenate documents into a single context\n",
    "    prompt = f\"Based on the following documents, answer the question: {query}\\n\\nDocuments:\\n{context}\\nAnswer:\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "answer = generate_answer_from_docs(user_query, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM-powered tools like **LlamaIndex** (formerly known as GPT Index) and **ChromaDB** are both used for managing and retrieving embeddings, but they have different use cases, capabilities, and architectures. Let's compare them in various aspects:\n",
    "\n",
    "### 1. **Purpose and Focus**:\n",
    "   - **LlamaIndex**:\n",
    "     - **Focus**: LlamaIndex is an orchestration layer built specifically for integrating large language models (LLMs) with external data sources. It helps connect LLMs to your data (documents, databases, APIs, etc.) to enable context-aware responses. Its primary role is to index and retrieve relevant chunks of information to feed into LLMs for specific queries, particularly when working with long documents or databases.\n",
    "     - **Use case**: It’s ideal when you want to enable LLMs to work with large or unstructured data sources, helping with tasks like document question-answering, summarization, and retrieval.\n",
    "  \n",
    "   - **ChromaDB**:\n",
    "     - **Focus**: ChromaDB is a vector database that specializes in storing, managing, and retrieving embeddings efficiently. It provides an API to interact with your embeddings and perform operations like similarity search or nearest neighbor search. ChromaDB is built to work with high-dimensional vectors, making it well-suited for tasks like recommendation systems, semantic search, and machine learning applications where embeddings play a critical role.\n",
    "     - **Use case**: Best for applications requiring fast and scalable embedding retrieval, such as semantic search, clustering, and recommendation systems.\n",
    "\n",
    "### 2. **Integration with LLMs**:\n",
    "   - **LlamaIndex**:\n",
    "     - **Designed for LLM integration**: One of its main strengths is its seamless integration with large language models. LlamaIndex structures the input data in a way that LLMs can efficiently query it and use the results for generating answers or performing tasks. It simplifies the process of ingesting data into LLM pipelines.\n",
    "     - **Advantages**: Offers tools for chunking large documents, organizing data hierarchically, and ensuring the LLM has the context it needs for more accurate responses.\n",
    "\n",
    "   - **ChromaDB**:\n",
    "     - **Embedding store**: ChromaDB’s primary focus is on efficient storage and retrieval of embeddings, but it doesn’t provide as deep an integration with LLMs by default. However, you can use ChromaDB to store and retrieve embeddings from LLM outputs, such as from OpenAI or other models, and use those embeddings for downstream tasks.\n",
    "     - **Advantages**: Optimized for high-performance search and retrieval of embeddings rather than directly serving LLMs.\n",
    "\n",
    "### 3. **Data Storage and Querying**:\n",
    "   - **LlamaIndex**:\n",
    "     - **Indexed data structure**: LlamaIndex creates specialized indices, such as tree-based or graph-based structures, that help retrieve context for a given query. It provides advanced tools for breaking down long documents into manageable chunks, which LLMs can query efficiently.\n",
    "     - **Querying**: LlamaIndex is designed to pull in relevant chunks of text or data when a query is made to the LLM, thus making it easier for models to deal with longer or more complex data.\n",
    "  \n",
    "   - **ChromaDB**:\n",
    "     - **Efficient vector storage**: ChromaDB is built as a vector database, focusing on fast and scalable operations such as similarity search across high-dimensional embeddings.\n",
    "     - **Querying**: ChromaDB specializes in performing quick vector similarity searches (e.g., cosine similarity, Euclidean distance) to retrieve the most similar embeddings for a given input, but it doesn’t directly work with textual chunks or documents in the way LlamaIndex does.\n",
    "\n",
    "### 4. **Handling of Large Data**:\n",
    "   - **LlamaIndex**:\n",
    "     - **Chunking and Summarization**: LlamaIndex is equipped to handle large datasets, especially large textual corpora. It breaks down long documents into smaller parts, making it easier for LLMs to access relevant information while bypassing token limitations.\n",
    "  \n",
    "   - **ChromaDB**:\n",
    "     - **Scaling with high-dimensional embeddings**: ChromaDB excels when working with large volumes of high-dimensional embeddings, but it doesn’t inherently provide document chunking, summarization, or contextual data for LLMs.\n",
    "\n",
    "### 5. **Customization and Flexibility**:\n",
    "   - **LlamaIndex**:\n",
    "     - **More LLM-centric flexibility**: Since LlamaIndex is designed for working with LLMs, it gives more flexibility in terms of how you structure data to provide context-aware responses. You can build various index types (e.g., list, tree, graph) depending on your use case.\n",
    "  \n",
    "   - **ChromaDB**:\n",
    "     - **Highly scalable vector database**: ChromaDB offers extensive flexibility for embedding storage, indexing, and retrieval. It is optimized for performance but doesn't offer the same level of LLM-specific customization that LlamaIndex provides.\n",
    "\n",
    "### 6. **Ease of Use**:\n",
    "   - **LlamaIndex**:\n",
    "     - **LLM-friendly**: It’s designed to work seamlessly with LLMs, making it easier for developers to build applications where LLMs require access to external knowledge.\n",
    "  \n",
    "   - **ChromaDB**:\n",
    "     - **Specialized tool**: It’s more specialized for applications where fast, efficient retrieval of embeddings is the key requirement, and it might require more work to integrate with LLMs manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, ServiceContext\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'your-openai-api-key'\n",
    "\n",
    "# Step 1: Load and prepare your data\n",
    "# Load documents from a directory\n",
    "def load_documents(directory_path):\n",
    "    return SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "# Step 2: Create the index using LlamaIndex\n",
    "def create_index(documents):\n",
    "    # Define the LLM Predictor using OpenAI's GPT model\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\"))\n",
    "    \n",
    "    # Create a service context (includes the LLM and settings)\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "    \n",
    "    # Build the index from the documents\n",
    "    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
    "    return index\n",
    "\n",
    "# Step 3: Query the index using an LLM\n",
    "def query_index(index, query):\n",
    "    response = index.query(query)\n",
    "    return response\n",
    "\n",
    "# Main function to run everything\n",
    "if __name__ == \"__main__\":\n",
    "    # Load documents from a local directory\n",
    "    documents = load_documents(\"path/to/your/documents\")\n",
    "\n",
    "    # Create the index\n",
    "    index = create_index(documents)\n",
    "\n",
    "    # Query the index with a user question\n",
    "    user_query = \"What are the main challenges in AI research?\"\n",
    "    response = query_index(index, user_query)\n",
    "\n",
    "    # Print the response from the LLM\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import GPTSimpleVectorIndex, GPTTreeIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'your-openai-api-key'\n",
    "\n",
    "# Step 1: Load documents\n",
    "def load_documents(directory_path):\n",
    "    return SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "# Step 2: Create a Vector Tree-based Index\n",
    "def create_tree_index(documents):\n",
    "    # Define the LLM Predictor (using OpenAI GPT)\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\"))\n",
    "    \n",
    "    # Create a service context\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "    \n",
    "    # Build the vector-based tree index from the documents\n",
    "    tree_index = GPTTreeIndex.from_documents(documents, service_context=service_context)\n",
    "    \n",
    "    return tree_index\n",
    "\n",
    "# Step 3: Query the Tree-based Index\n",
    "def query_tree_index(tree_index, query):\n",
    "    response = tree_index.query(query)\n",
    "    return response\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Load documents from your directory\n",
    "    documents = load_documents(\"path/to/your/documents\")\n",
    "    \n",
    "    # Create the vector tree-based index\n",
    "    tree_index = create_tree_index(documents)\n",
    "\n",
    "    # Query the index\n",
    "    user_query = \"What are the main challenges in AI research?\"\n",
    "    response = query_tree_index(tree_index, user_query)\n",
    "\n",
    "    # Output the response\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "writer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
